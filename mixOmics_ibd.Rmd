---
title: "mixOmics analysis of PSC patients regarding IBD"
author: "William Wu"
date: "2024-07-25"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background

In this analysis we are using N-integration from the mixOmics package to integrate high-throughput miRNA-, protein-, and metabolite data. The data types are layered upon each other creating a three dimensional block. The algorithm used is block PLS-DA where block indicates the multiple data layers, PLS (partial least squares) indicates the algorithm which is similar to PCA, and DA (discriminant analysis) indicates the dependent variable as categorical.

The data is divided into a training and testing subset which will be used to create and validate the model respectively. The outcome variable is categorical. To visualise the results we will plot the grouped individuals and the relationship between the variables.

The parameters that are varied and tweaked are included in **Parameter choice**. They consist of the weight of the design matrix, the number of components, and the number of variables.

# Analysis

## Read the libraries

```{r read_the_libraries}
library(tidyverse)
library(mixOmics)
```

## Load the data

The omics data comes in a uniform format with each row representing a patient/sample and each column representing a compound/feature/variable except the first column which contains the patient ID's.

The metadata has been cleaned for the variables of interest so that they are represented by new binary columns. See the preprocessing script for details.

```{r load_the_data}
load("~/R/FoL채k2/results/metabolite_preprocessed.RData")

load("~/R/FoL채k2/results/protein_preprocessed.RData")

load("~/R/FoL채k2/results/miRNA_preprocessed.RData")

load("~/R/FoL채k2/results/metadata_preprocessed.RData")
```

## Prepare the data

We prepare the data by selecting the PSC patients (n = 33) and filtering the variables with low variance using nearZeroVar. Thereafter we split the data into a training and testing set. The latter consists of two patients with IBD (n = 1, 2) and two patients without IBD (n = 12, 14).

```{r prepare_the_data}
# Assigning test patients by looking in the metadata

test_sample <- c(1, 2, 12, 14)

# miRNA

miRNA_psc <- miRNA_preprocessed %>% 
  dplyr::select(-1) %>% 
  slice_head(n = 33)

miRNA_nzv <- miRNA_psc %>% 
  nearZeroVar(uniqueCut = 30)

miRNA_variance <- miRNA_psc[,-miRNA_nzv$Position]

# Splitting data

miRNA_train <- miRNA_variance %>% 
  slice(-test_sample) %>% 
  as.matrix

miRNA_test <- miRNA_variance %>% 
  slice(test_sample) %>% 
  as.matrix


# Protein

protein_psc <- protein_preprocessed %>% 
  dplyr::select(-1) %>% 
  slice_head(n = 33)

# Splitting the data

protein_train <- protein_psc %>% 
  slice(-test_sample) %>% 
  as.matrix

protein_test <- protein_psc %>% 
  slice(test_sample) %>% 
  as.matrix


# Metabolite

metabolite_psc <- metabolite_preprocessed %>% 
  dplyr::select(-1) %>% 
  slice_head(n = 33)

metabolite_nzv <- metabolite_psc %>% 
  nearZeroVar(uniqueCut = 30)

metabolite_variance <- metabolite_psc[,-metabolite_nzv$Position]

# Splitting the data

metabolite_train <- metabolite_variance %>% 
  slice(-test_sample) %>% 
  as.matrix

metabolite_test <- metabolite_variance %>% 
  slice(test_sample) %>% 
  as.matrix


# Metadata

metadata_psc <- metadata_preprocessed %>% 
  slice_head(n = 33)

# Splitting the data

metadata_train <- metadata_psc %>% 
  slice(-test_sample)

metadata_test <- metadata_psc %>% 
  slice(test_sample)


# Assigning the block and outcome variable for train and test

X_train <- list(
  miRNA = miRNA_train,
  protein = protein_train,
  metabolite = metabolite_train
)

Y_train <- metadata_train$ibd_binary

X_test <- list(
  miRNA = miRNA_test,
  protein = protein_test,
  metabolite = metabolite_test
)

Y_test <- metadata_test$ibd_binary
```

## Choose parameters

### Design matrix

We start by investigating the pair-wise correlation between the different layers in our block using the PLS algorithm as regression analysis. These values can later be used to compare with the results of plotDiablo().

```{r correlation_analysis}
# miRNA x Protein

res1.pls <- pls(X_train$miRNA, X_train$protein, ncomp = 1)
cor(res1.pls$variates$X, res1.pls$variates$Y)

# miRNA x Metabolite

res2.pls <- pls(X_train$miRNA, X_train$metabolite, ncomp = 1)
cor(res2.pls$variates$X, res2.pls$variates$Y)

# Protein x Metabolite

res3.pls <- pls(X_train$protein, X_train$metabolite, ncomp = 1)
cor(res3.pls$variates$X, res3.pls$variates$Y)
```

Thereafter we create the design matrix with where a lower weight gives higher prediction accuracy while a higher weight extracts the correlation structure more precisely.

```{r create_design_matrix}
design <- matrix(
  0.1, 
  nrow = length(X_train), 
  ncol = length(X_train), 
  dimnames = list(names(X_train), names(X_train))
  )

diag(design) <- 0
```

### Number of components

To find the optimal number of components I create a initial model with block.plsda() and analyse it with perf(). The arguments are set in accordance with the mixOmics vignette.

```{r calculate_number_of_components}
# # Finding the global performance of the model without variable selction
# 
# diablo.initial <- block.plsda(X_train, Y_train, ncomp = 5, design = design)
# 
# # 10-fold cross validating the model with 10 repeats
# 
# perf.diablo.initial <- perf(diablo.initial, validation = "Mfold", folds = 10, nrepeat = 10)
# 
# # Plotting the error rate
# 
# plot(perf.diablo.initial)
# 
# # Getting the number of components according to majority vote
# 
# perf.diablo.initial$choice.ncomp$WeightedVote
```

Since the outcome variable is binary the optimal number of components will usually be 1; it is usually the number of distinct outcomes minus 1. Therefore I manually set the components to 2 which is the minimum amount to be plotted.

```{r assign_number_of_components}
#ncomp = perf.diablo$choice.ncomp$WeightedVote["Overall.BER", "centroids.dist"]

ncomp <- 2
```

### Number of variables

To find the optimal number of variables I create a grid for the span of variables to keep for each omics which can be adjusted and optimized depending on the amount of features contained. Thereafter I analyse the data with tune.block.splsda which gives me an optimal number of variables per layer and component.

The most important parameters to tweak are test.keepX (start coarse go fine) and nrepeat (generally 10-50 repeats) and the form of validation.

```{r calculate_number_of_variables}
# # Selecting a range of amount of variables to keep and storing them as a list
# 
# test.keepX <- list(
#                    miRNA = c(5:9, seq(10, 110, 20)),
#                    protein = c(5:9, seq(10, 110, 20)),
#                    metabolite = c(5:9, seq(10, 110, 20))
#                   )
# 
# # Tuning with crossvalidation, i.e. choosing the variables to keep
# 
# start <- Sys.time()
# tune.diablo <- tune.block.splsda(X_train, Y_train, ncomp = ncomp,
#                               test.keepX = test.keepX, design = design,
#                               validation = 'loo', folds = 10, nrepeat = 10,
#                               BPPARAM = BiocParallel::SnowParam(workers = 8),
#                               dist = "centroids.dist")
# end <- Sys.time()
# print(end - start)
# 
# list.keepX <- tune.diablo$choice.keepX
```

Here I save the results of previous calculations as they take some time to make and may vary.

```{r assign_number_of_variables}
list.keepX <- list(
  miRNA = c(20, 5),
  protein = c(15, 15),
  metabolite = c(5, 5)
)

# test.keepX <- list(
#                    miRNA = c(5:9, seq(10, 25, 5)),
#                    protein = c(5:9, seq(10, 25, 5)),
#                    metabolite = c(5:9, seq(10, 25, 5))
#                   )

# 20, 5; 15, 15; 5, 5; nrepeat = 10, workers = 8, time = 6,94 min

# test.keepX <- list(
#                    miRNA = c(5:9, seq(10, 50, 10)),
#                    protein = c(5:9, seq(10, 50, 10)),
#                    metabolite = c(5:9, seq(10, 50, 10))
#                   )

# 50, 5; 50, 30; 5, 5; nrepeat = 10, workers = 8, time = 9,59 min

# test.keepX <- list(
#                    miRNA = c(5:9, seq(10, 110, 20)),
#                    protein = c(5:9, seq(10, 110, 20)),
#                    metabolite = c(5:9, seq(10, 110, 20))
#                   )

# 70, 5; 50, 8; 5, 5; loo, workers = 8, time = 12,69 min
```

## Create final model

We create the final model with the training data set and the three parameters that we have defined.

```{r create_final_model}
diablo.final <- block.splsda(X_train, Y_train, keepX = list.keepX, ncomp = ncomp, design = design)
```

# Results

## Performance

We investigate the error rate of our final model and plot AUC. Here it can be important to choose the right type of validation and amount of folds

```{r analyse_performance}
perf.diablo.final <- perf(diablo.final,  validation = 'loo', folds = 10, 
                         nrepeat = 10, dist = 'centroids.dist')

# Performance with Majority vote

perf.diablo.final$MajorityVote.error.rate

# Performance with Weighted vote

perf.diablo.final$WeightedVote.error.rate

# AUC plot per block

for (i in c("miRNA", "protein", "metabolite")) {
  auc.diablo.final <- auroc(diablo.final, roc.block = i, roc.comp = 2,
                           print = FALSE)
}
```

## Prediction

We use our final model to predict the outcome for the test data set and then compare to the actual outcome.

```{r analyse_prediction}
predict.diablo.final <- predict(diablo.final, newdata = X_test)

confusion.mat <- get.confusion_matrix(truth = Y_test, 
                     predicted = predict.diablo.final$WeightedVote$centroids.dist[,2])

dimnames(confusion.mat) <- list(
  c("no IBD", "IBD"),
  c("predicted as no IBD", "predicted as IBD")
)  

confusion.mat

get.BER(confusion.mat)
```

## Plot correlation

We plot the correlation between the layers.

```{r plot_correlation}
for (i in 1:2) {
  plotDiablo(diablo.final, ncomp = i)
}
```

## Plot individuals

```{r plot_individuals}
# Individual plot

plotIndiv(diablo.final, ind.names = FALSE, legend = TRUE, 
          title = 'DIABLO comp 1 - 2')

# Arrow plot

plotArrow(diablo.final, ind.names = FALSE, legend = TRUE, 
          title = 'DIABLO comp 1 - 2')
```

## Plot variables

```{r plot_variables}
# Variable plot

plotVar(diablo.final, var.names = FALSE, style = 'graphics', legend = TRUE, 
        pch = c(16, 17, 15), cex = c(2,2,2), 
        col = c('darkorchid', 'brown1', 'lightgreen'),
        title = 'TCGA, DIABLO comp 1 - 2')

# Loadings


for (i in 1:2) {
  plotLoadings(diablo.final, comp = i, contrib = 'max', method = 'median')
}

# Circos plot

circosPlot(diablo.final, cutoff = 0.7, line = TRUE, 
           color.blocks = c('darkorchid', 'brown1', 'lightgreen'),
           color.cor = c("chocolate3","grey20"), size.labels = 1.5)

# # Network
# 
# # X11()   # Opens a new window
# network(diablo.final, blocks = c(1,2,3), 
#         cutoff = 0.4,
#         color.node = c('darkorchid', 'brown1', 'lightgreen'),
#         # To save the plot, uncomment below line
#         #save = 'png', name.save = 'diablo-network'
#         )
# 
# # Clustered image map
# 
# cimDiablo(diablo.final, color.blocks = c('darkorchid', 'brown1', 'lightgreen'),
#           comp = 1, margin=c(8,20), legend.position = "right")
```

# Conclusion

As the prediction accuracy is fairly low we have to tweak the parameters, specifically the number of variables, the design matrix, and the validation parameters in the functions.
